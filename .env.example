#
# LLM APIs settings 
#

# Ollama is one of the simplest ways to get started running models locally: https://ollama.ai/
OLLAMA_API_URL="http://localhost:11434"
#OPENAI_API_KEY="" 
#ANTHROPIC_API_KEY=""
#COHERE_API_KEY=""
#PERPLEXITYAI_API_KEY=""
