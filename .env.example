#
# General settings 
#
VECTOR_SEARCH_PATH="./chromadb"
VECTOR_SEARCH_COLLECTION_NAME="collection"

VECTOR_SEARCH_SENTENCE_TRANSFORMER_MODEL="intfloat/e5-large-v2" # Can be any Sentence-Transformers compatible model available on Hugging Face
VECTOR_SEARCH_SENTENCE_TRANSFORMER_DEVICE="cpu"
VECTOR_SEARCH_DISTANCE_FUNCTION="cosine" # https://docs.trychroma.com/usage-guide#changing-the-distance-function
VECTOR_SEARCH_NORMALIZE_EMBEDDINGS="true"
VECTOR_SEARCH_CHUNK_PREFIX="passage: " # Can be used to add prefix to text embeddings stored in vector store
VECTOR_SEARCH_QUERY_PREFIX="query: " # Can be used to add prefix to text embeddings used for semantic search

VECTOR_SEARCH_TEXT_SPLITTER_CHUNK_OVERLAP=25 # Determines, for a given chunk of text, how many tokens must overlap with adjacent chunks.
VECTOR_SEARCH_SEARCH_N_RESULTS=4 # How many entries should the vector search return?

#
# Summarization settings
#
SUMMARIZATION_MODEL = "ollama/mixtral" # CAUTION: Using a remote model can be costly!

SUMMARIZATION_MAX_TOKENS = 450

SUMMARIZATION_MODEL_TEMPERATURE = 0.2

SUMMARIZATION_EXCERPT_TEMPLATE = "AI-generated summary of {case_name} from {court_full_name}." # {case_name} and {court_full_name} as reserved keywords.

#
# Prompts
#
SUMMARIZATION_PROMPT = "Summarize the following court opinion:"

RAG_PROMPT="
Here is context:
{context}
----------------
TODO

Question: {question}

Helpful answer:"
# Inspired by LangChain's default prompt. {context} and {question} are reserved keywords.

#
# LLMs API keys 
#
#OPENAI_API_KEY="" 
#ANTHROPIC_API_KEY=""
#COHERE_API_KEY=""
#PERPLEXITYAI_API_KEY=""
OLLAMA_API_URL="http://localhost:11434"

#
# Hugging Face's tokenizer settings
#
TOKENIZERS_PARALLELISM="false"
